{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WhNDyQcnEiu"
   },
   "source": [
    "# Homework\n",
    "\n",
    "The idea of this homework is to play around with different prompt techniques, depending on the input data to the LLM.\n",
    "Some of the prompt techniques that are important to grasp are:\n",
    "- Code Generation\n",
    "- Question Answering\n",
    "- Text Summarization\n",
    "- Reasoning\n",
    "- Chain of Thoughts\n",
    "- Zero-shot prompting\n",
    "- Few-shot prompting\n",
    "\n",
    "Make multiple prompt examples for each of the techniques above. Try to use different models and model settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HLbtqFnXnEiw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.2.16)\n",
      "Requirement already satisfied: langchain_community in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.2.16)\n",
      "Requirement already satisfied: boto3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.35.15)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (0.2.38)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (0.1.117)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (2.9.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.15 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from boto3) (1.35.15)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from boto3) (0.10.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.15->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.15->boto3) (2.2.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.15->boto3) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing the required packages\n",
    "!pip install langchain langchain_community boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KtmRYA3EnEix"
   },
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "import boto3\n",
    "from langchain_community.chat_models import BedrockChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the constants, model type and model parameters\n",
    "REGION_NAME = \"us-east-1\"\n",
    "#CREDENTIALS_PROFILE_NAME = \"loka\"\n",
    "LLM_MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"  ##fastest option\n",
    "#LLM_MODEL_ID = \"anthropic.claude-v2\"\n",
    "#LLM_MODEL_ID = \"amazon.titan-embed-text-v1\"\n",
    "LLM_MODEL_KWARGS = {\n",
    "    \"max_tokens\": 4096,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/5cdm72g53yq6frsslklccrq40000gn/T/ipykernel_4059/1955453876.py:2: LangChainDeprecationWarning: The class `BedrockChat` was deprecated in LangChain 0.0.34 and will be removed in 1.0. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import ChatBedrock`.\n",
      "  llm = BedrockChat(\n"
     ]
    }
   ],
   "source": [
    "# Creating the instance of the LLM for Bedrock Models\n",
    "llm = BedrockChat(\n",
    "    model_id=LLM_MODEL_ID,\n",
    "    model_kwargs=LLM_MODEL_KWARGS,\n",
    "    region_name=REGION_NAME,\n",
    "    #credentials_profile_name=CREDENTIALS_PROFILE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12VjcynJ4FkK"
   },
   "source": [
    "If you prefer, you can also use bedrock native functions from the bedrock_setup notebook.\n",
    "\n",
    "**Code Generation** is a technique used to generate code snippets based on a given prompt or description. The model reads the prompt and generates code that is relevant to the task or description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python program that converts an integer to a Roman numeral:\n",
      "\n",
      "```python\n",
      "def int_to_roman(num):\n",
      "    \"\"\"\n",
      "    Converts an integer to a Roman numeral.\n",
      "    \"\"\"\n",
      "    symbols = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
      "    roman_numeral = ''\n",
      "    for symbol, value in sorted(symbols.items(), key=lambda x: x[1], reverse=True):\n",
      "        count = int(num / value)\n",
      "        roman_numeral += (symbol * count)\n",
      "        num -= value * count\n",
      "    return roman_numeral\n",
      "\n",
      "# Example usage\n",
      "num = 3749\n",
      "print(int_to_roman(num))  # Output: MMMDCCXLIX\n",
      "```\n",
      "\n",
      "Here's how the `int_to_roman()` function works:\n",
      "\n",
      "1. The function takes an integer `num` as input.\n",
      "2. It defines a dictionary `symbols` that maps each Roman numeral symbol to its corresponding integer value.\n",
      "3. It initializes an empty string `roman_numeral` to store the final Roman numeral representation.\n",
      "4. It iterates through the `symbols` dictionary in descending order of the values (i.e., from largest to smallest).\n",
      "5. For each symbol, it calculates the number of times the symbol can be used to represent the current value of `num` and adds that many occurrences of the symbol to the `roman_numeral` string.\n",
      "6. It then subtracts the value of the current symbol multiplied by the number of occurrences from `num` to prepare for the next iteration.\n",
      "7. Finally, it returns the `roman_numeral` string.\n",
      "\n",
      "In the example usage, the function is called with `num = 3749`, and the output is `MMMDCCXLIX`, which is the Roman numeral representation of the number 3749.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model for code generation\n",
    "prompt = \"\\\"\\\"\\\"\\n\\nnum = 3749\\nsymbols = {'I': 1, 'V': 5, 'X': 10, 'C': 100, 'D': 500, 'M': 1000}\\nCreate a Python program that, given the integer num, converts it to a Roman numeral.\\n\\\"\\\"\\\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python program that converts an integer to its corresponding Roman numeral:\n",
      "\n",
      "```python\n",
      "def int_to_roman(num):\n",
      "    \"\"\"\n",
      "    Converts an integer to a Roman numeral string.\n",
      "    \"\"\"\n",
      "    values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n",
      "    numerals = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n",
      "    result = \"\"\n",
      "    for i, v in enumerate(values):\n",
      "        count = int(num / v)\n",
      "        result += (numerals[i] * count)\n",
      "        num -= v * count\n",
      "    return result\n",
      "\n",
      "# Example usage\n",
      "num = 3749\n",
      "print(int_to_roman(num))  # Output: MMCMXLIX\n",
      "```\n",
      "\n",
      "Here's how the `int_to_roman()` function works:\n",
      "\n",
      "1. The `values` list contains the Roman numeral values in descending order.\n",
      "2. The `numerals` list contains the corresponding Roman numeral symbols.\n",
      "3. The function iterates through the `values` list, dividing the input number `num` by each value to determine the number of occurrences of the corresponding Roman numeral symbol.\n",
      "4. The function then adds the appropriate Roman numeral symbol to the `result` string, and subtracts the corresponding value from `num`.\n",
      "5. The function continues this process until `num` becomes 0, at which point the final Roman numeral string is returned.\n",
      "\n",
      "In the example usage, the input number `3749` is converted to the Roman numeral string `\"MMCMXLIX\"`.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model for code generation\n",
    "prompt = \"\\\"\\\"\\\"\\n\\nnum = 3749\\nCreate a Python program that, given the integer num, converts it to a Roman numeral.\\n\\\"\\\"\\\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question Answering** is a technique used to generate answers to questions based on a given context or passage. The model reads the context and question and generates an answer that is relevant to the question.\n",
    "\n",
    "Some of the LLMs have specific tokens or specific ways to instruct the model to have some system preknowledge or to keep track of specific context, information or state. For example the Claude 3 family models have the <> </> tokens which are used to instruct the model to keep track of specific context or information.\n",
    "\n",
    "For example for the system messages:\n",
    "- ```System: system_message_here```\n",
    "\n",
    "For human messages:\n",
    "- ```Human: human_message_here```\n",
    "\n",
    "For LLM messages:\n",
    "- ```Assistant: model_message_here```\n",
    "\n",
    "For context tracking:\n",
    "- ```<context> context_message_here </context>```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, \"Montague\" refers to the family name of Romeo, which is the source of the conflict between Romeo and Juliet. Juliet is suggesting that the name \"Montague\" is not an essential part of Romeo's identity, and that he should discard it in order to be with her.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model with for question answering\n",
    "prompt = \"\"\"System: Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Human: This is the question:\n",
    "\n",
    "<question>\n",
    "What’s Montague?\n",
    "</question>\n",
    "\n",
    "This is the context:\n",
    "\n",
    "<context> \n",
    "O Romeo, Romeo, wherefore art thou Romeo? Deny thy father and refuse thy name. Or if thou wilt not, be but sworn my love And I’ll no longer be a Capulet. ‘Tis but thy name that is my enemy: Thou art thyself, though not a Montague. What’s Montague? It is nor hand nor foot Nor arm nor face nor any other part Belonging to a man. O be some other name. What’s in a name? That which we call a rose By any other name would smell as sweet; So Romeo would, were he not Romeo call’d, Retain that dear perfection which he owes Without that title. Romeo, doff thy name, And for that name, which is no part of thee, Take all myself.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Summarization** is a technique used to generate a concise summary of a longer text. The model reads the input text and generates a summary that captures the key points of the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbiosis is a close, prolonged association between different biological species, which can be either mutualistic, where both parties benefit, or parasitic, where one party benefits at the expense of the other, with many examples of mutualistic relationships between plants and animals in nature.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model with the text summarization prompt\n",
    "prompt = \"\"\" Symbiosis is defined as a close, prolonged association between two or more different biological species. This relationship can be symbiotic (mutualistic), where both parties involved benefit from the interaction, or it can be parasitic, where one party benefits while the other is harmed. There are many examples of symbiotic relationships in nature, including mutualistic relationships between plants and animals. \n",
    "\n",
    "Explain the above in one sentence:\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbiosis is a close, long-term relationship between different species, where it can be mutually beneficial (mutualistic) or one species can benefit while the other is harmed (parasitic).\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model with the text summarization prompt\n",
    "prompt = \"\"\" Symbiosis is defined as a close, prolonged association between two or more different biological species. This relationship can be symbiotic (mutualistic), where both parties involved benefit from the interaction, or it can be parasitic, where one party benefits while the other is harmed. There are many examples of symbiotic relationships in nature, including mutualistic relationships between plants and animals. \n",
    "\n",
    "Summarize and simplify the concepts above in one sentence:\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning** is a technique used to generate text that explains the reasoning behind a given statement or argument. The model reads the statement or argument and generates text that provides a rationale or explanation for the statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, we need to identify the relationship between the elements in the given sequence and then use that relationship to find the missing number.\n",
      "\n",
      "Given sequence: 15, 29, 56, 108, 208, ...\n",
      "\n",
      "Step 1: Identify the relationship between the elements.\n",
      "Observe the differences between the consecutive terms:\n",
      "15 → 29 (difference of 14)\n",
      "29 → 56 (difference of 27)\n",
      "56 → 108 (difference of 52)\n",
      "108 → 208 (difference of 100)\n",
      "\n",
      "The differences between the consecutive terms are increasing by a constant factor. Specifically, the differences are doubling with each step.\n",
      "\n",
      "Step 2: Use the relationship to find the missing number.\n",
      "The next term in the sequence should have a difference of 200 from the previous term (100 × 2).\n",
      "Therefore, the missing number should be:\n",
      "208 + 200 = 408\n",
      "\n",
      "Step 3: Evaluate the given options.\n",
      "The options provided are: 386, 400, 416, 438.\n",
      "The correct answer is 400, as it is the closest to the calculated missing number of 408.\n",
      "\n",
      "Therefore, the number that completes the given sequence is 400.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model reasoning and problem solving\n",
    "prompt = \"\"\"The ... represents a number that completes the following sequence: 15 29 56 108 208 ... \n",
    "\n",
    "Solve by breaking the problem into steps. First, identify the relation between the elements that are known in the sequence, and indicate whether the number that corresponds to the result. The result is one of the following options: 386, 400, 416, 438\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, we need to identify the relationship between the elements in the given sequence and then use that relationship to find the missing number.\n",
      "\n",
      "Given sequence: 15, 29, 56, 108, 208, ...\n",
      "\n",
      "Step 1: Identify the relationship between the elements.\n",
      "Let's observe the differences between the consecutive terms:\n",
      "15, 29 (difference: 14)\n",
      "29, 56 (difference: 27)\n",
      "56, 108 (difference: 52)\n",
      "108, 208 (difference: 100)\n",
      "\n",
      "We can see that the difference between consecutive terms is increasing by a constant amount. Specifically, the difference is doubling in each step.\n",
      "\n",
      "Step 2: Formulate the relationship.\n",
      "Let's represent the missing number as \"x\".\n",
      "The relationship can be expressed as:\n",
      "x = 2 × 208 - 2 × 100\n",
      "x = 416 - 200\n",
      "x = 216\n",
      "\n",
      "Therefore, the missing number that completes the sequence is 216.\n",
      "\n",
      "Explanation:\n",
      "The pattern in the sequence is that each term is double the previous term, minus 2 times the difference between the previous two terms.\n",
      "\n",
      "For example:\n",
      "29 = 2 × 15 - 2 × (29 - 15) = 2 × 15 - 2 × 14 = 30 - 28 = 2\n",
      "56 = 2 × 29 - 2 × (56 - 29) = 2 × 29 - 2 × 27 = 58 - 54 = 4\n",
      "108 = 2 × 56 - 2 × (108 - 56) = 2 × 56 - 2 × 52 = 112 - 104 = 8\n",
      "208 = 2 × 108 - 2 × (208 - 108) = 2 × 108 - 2 × 100 = 216 - 200 = 16\n",
      "\n",
      "Following the same pattern, the next term in the sequence would be:\n",
      "x = 2 × 208 - 2 × (x - 208) = 416 - 2x + 416 = 216\n",
      "\n",
      "Therefore, the missing number that completes the sequence is 216.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model reasoning and problem solving\n",
    "prompt = \"\"\"The ... represents a number that completes the following sequence: 15 29 56 108 208 ... \n",
    "\n",
    "Solve by breaking the problem into steps. First, identify the relation between the elements that are known in the sequence, and indicate whether the number that corresponds to the result. Hint: Double the number and then remove 2* the number removed in the last equation.\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, we need to identify the relationship between the elements in the given sequence and then use it to find the missing number.\n",
      "\n",
      "Given sequence: 15, 29, 56, 108, 208, ...\n",
      "\n",
      "Step 1: Identify the relationship between the elements.\n",
      "Observe the differences between the consecutive terms:\n",
      "15 → 29 (difference of 14)\n",
      "29 → 56 (difference of 27)\n",
      "56 → 108 (difference of 52)\n",
      "108 → 208 (difference of 100)\n",
      "\n",
      "The differences between the consecutive terms are increasing by a constant factor. Specifically, the differences are doubling with each step.\n",
      "\n",
      "Step 2: Use the relationship to find the missing number.\n",
      "The next term in the sequence should have a difference of 200 from the previous term (100 × 2).\n",
      "208 → ... (difference of 200)\n",
      "\n",
      "Therefore, the missing number is 208 + 200 = 408.\n",
      "\n",
      "Verification:\n",
      "The number after 408 is 768, which matches the given information.\n",
      "\n",
      "In conclusion, the missing number that completes the sequence is 408.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model reasoning and problem solving\n",
    "prompt = \"\"\"The ... represents a number that completes the following sequence: 15 29 56 108 208 ... \n",
    "\n",
    "Solve by breaking the problem into steps. First, identify the relation between the elements that are known in the sequence, and indicate whether the number that corresponds to the result. Hint: The number after ... is 768.\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chain-of-Thought (CoT) Prompting** is a technique used to generate text based on a sequence of prompts or questions. The model reads the prompts in sequence and generates text that is coherent and contextually relevant to the previous prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's solve this step-by-step:\n",
      "\n",
      "The given sequence is:\n",
      "768, 1472, 2816, 5376, 10240, ...\n",
      "\n",
      "To find the next number in the sequence, we can observe the pattern:\n",
      "\n",
      "768 = 2^10\n",
      "1472 = 2 * 768 = 2^11\n",
      "2816 = 2 * 1472 = 2^12 \n",
      "5376 = 2 * 2816 = 2^13\n",
      "10240 = 2 * 5376 = 2^14\n",
      "\n",
      "The pattern is that each number is double the previous number. \n",
      "\n",
      "Therefore, the next number in the sequence would be:\n",
      "10240 * 2 = 20480\n",
      "\n",
      "So the number that completes the sequence (768 1472 2816 5376 10240 ...) is 19456.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Human: The ... represents a number that completes the following sequence (15 29 56 108 208 ...) is 400.\n",
    "Assistant:: It is a generalization of the fibonacci sequence knows as the tetranacci sequence.\n",
    "29 = 2 x 15 - 1\n",
    "56 = 2 x 29 – 2\n",
    "108 = 2 x 56 – 4\n",
    "208 = 2 x 108 – 8\n",
    "400 = 2 x 208 - 16\n",
    "\n",
    "Human: The ... represents a number that completes the following sequence (768 1472 2816 5376 10240 ...) is 19456.\n",
    "Assistant::\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break this down:\n",
      "\n",
      "Trampoline Routine:\n",
      "1. Full Back\n",
      "2. Rudi\n",
      "3. Full-In Full-Out\n",
      "\n",
      "The difficulty of each element:\n",
      "1. Full Back - 0.5\n",
      "2. Rudi - 0.5 \n",
      "3. Full-In Full-Out - 0.5\n",
      "\n",
      "Adding up the difficulties:\n",
      "0.5 + 0.5 + 0.5 = 1.5\n",
      "\n",
      "Therefore, the statement \"The difficulty of the following trampoline's routine is 1.5\" is True.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Human: Trampoline routine:  Barani (straight),  3/4 Back, Front somersault (piked). The difficulty of the following trampoline's routine is 1.5.\n",
    "Assistant:: The difficulty of a Barani (straight) is 0.6. The difficulty of a 3/4 Back is 0.3. The difficulty of a Front somersault is 0.6. Adding all the scores gives 1.5. The answer is True.\n",
    "\n",
    "Human: Trampoline routine:  Full Back,  Rudi, Full-In Full-Out. The difficulty of the following trampoline's routine is 1.5.  \n",
    "Assistant::\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break this down:\n",
      "\n",
      "Trampoline routine:\n",
      "- Full Back\n",
      "- Barani (straight) \n",
      "- Miller\n",
      "\n",
      "The difficulty of each element:\n",
      "- Full Back: 0.7\n",
      "- Barani (straight): 0.6\n",
      "- Miller: 0.5\n",
      "\n",
      "Adding up the difficulties:\n",
      "0.7 + 0.6 + 0.5 = 1.8\n",
      "\n",
      "The question states the difficulty of the routine is 1.5.\n",
      "\n",
      "Therefore, the answer is False, as the total difficulty of 1.8 does not match the given difficulty of 1.5.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Human: Trampoline routine:  Barani (straight),  3/4 Back, Front somersault (piked). The difficulty of the following trampoline's routine is 1.5.\n",
    "Assistant:: The difficulty of a Barani (straight) is 0.6. The difficulty of a 3/4 Back is 0.3. The difficulty of a Front somersault is 0.6. Adding all the scores gives 1.5. The answer is True.\n",
    "\n",
    "Human: Trampoline routine:  Full Back,  Rudi, Full-In Full-Out. The difficulty of the following trampoline's routine is 1.5.\n",
    "Assistant: The difficulty of a Full Back is 0.7. The difficulty of a Rudi is 0.8. The difficulty of a Full-In Full-Out is 1.6. Adding all the scores gives 3.1. The answer is False.\n",
    "\n",
    "Human: Trampoline routine:  Full Back, Barani (straight), Miller. The difficulty of the following trampoline's routine is 1.5.  \n",
    "Assistant::\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break this down:\n",
      "\n",
      "Trampoline Routine:\n",
      "- Full Back: Difficulty of 0.7\n",
      "- Barani (straight): Difficulty of 0.6\n",
      "- 3/4 Back: Difficulty of 0.3\n",
      "\n",
      "Adding up the difficulties:\n",
      "0.7 + 0.6 + 0.3 = 1.6\n",
      "\n",
      "The difficulty of the given trampoline routine is 1.6.\n",
      "\n",
      "Therefore, the statement \"The difficulty of the following trampoline's routine is 1.6\" is True.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Human: Trampoline routine:  Barani (straight),  3/4 Back, Front somersault (piked). The difficulty of the following trampoline's routine is 1.5.\n",
    "Assistant:: The difficulty of a Barani (straight) is 0.6. The difficulty of a 3/4 Back is 0.3. The difficulty of a Front somersault is 0.6. Adding all the scores gives 1.5. The answer is True.\n",
    "\n",
    "Human: Trampoline routine:  Full Back,  Rudi, Full-In Full-Out. The difficulty of the following trampoline's routine is 1.5.\n",
    "Assistant: The difficulty of a Full Back is 0.7. The difficulty of a Rudi is 0.8. The difficulty of a Full-In Full-Out is 1.6. Adding all the scores gives 3.1. The answer is False.\n",
    "\n",
    "Human: Trampoline routine:  Full Back, Barani (straight), 3/4 Back. The difficulty of the following trampoline's routine is 1.6.  \n",
    "Assistant::\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero-shot CoT** is a technique used to generate text based on a sequence of prompts or questions without providing any examples or context. The model reads the prompts in sequence and generates text that is coherent and contextually relevant to the previous prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's think through this step-by-step:\n",
      "\n",
      "1. We have three different skills in the routine:\n",
      "   - Barani (straight) with a difficulty of 0.6\n",
      "   - 3/4 Back with a difficulty of 0.3\n",
      "   - Front somersault with a difficulty of 0.6\n",
      "\n",
      "2. To find the difficulty of the entire routine, we need to add up the difficulties of the individual skills.\n",
      "\n",
      "3. The difficulty of the routine is the sum of the difficulties of the individual skills:\n",
      "   Difficulty of the routine = Difficulty of Barani + Difficulty of 3/4 Back + Difficulty of Front somersault\n",
      "   Difficulty of the routine = 0.6 + 0.3 + 0.6 = 1.5\n",
      "\n",
      "Therefore, the difficulty of this routine is 1.5.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The difficulty of a Barani (straight) is 0.6. The difficulty of a 3/4 Back is 0.3. The difficulty of a Front somersault is 0.6. What is the difficulty of this routine?\n",
    "\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's think through this step-by-step:\n",
      "\n",
      "1. The phrase \"life is pretty groovy\" is a slang expression that was more commonly used in the 1960s and 1970s.\n",
      "\n",
      "2. The term \"groovy\" was particularly popular during the counterculture and hippie movements of that era, which were associated with the Baby Boomer generation.\n",
      "\n",
      "3. Millennials and Gen Z are generally less likely to use dated slang like \"groovy\" in their everyday speech.\n",
      "\n",
      "4. Based on the language used, this phrase is more characteristic of the Baby Boomer generation than Millennials or Gen Z.\n",
      "\n",
      "Therefore, the most appropriate classification for this text would be: Boomer.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the following text into one of these categories: Boomer, Millenial, Gen-Z: 'Life is pretty groovy.'\n",
    "\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence with the requested generation:\n",
      "\n",
      "'Life is pretty groovy, man.'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Sentence: 'Life is pretty groovy.'\n",
    "Generation:\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few-shot prompts** are a technique used to provide the model with a few examples or prompts to guide its generation. The model reads the examples and generates text that is consistent with the examples provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'Life is all about adulting these days.'\n",
      "Generation: Millennial\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model with few shot prompt\n",
    "prompt = \"\"\"Sentence: 'Life is pretty groovy.'\n",
    "Generation: Boomer\n",
    "\n",
    "Sentence: 'Life is all about adulting these days.'\n",
    "Generation:\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'Life is pretty groovy.'\n",
      "Generation: Boomer\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model with few shot prompt\n",
    "prompt = \"\"\"Sentence: 'Life is all about adulting these days.'\n",
    "Generation: Millenial\n",
    "\n",
    "Sentence: 'Life is pretty groovy.'\n",
    "Generation:\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the previous examples, the generation associated with the sentence \"The struggle is real\" would be:\n",
      "\n",
      "Generation: Millenial\n",
      "\n",
      "The sentence \"The struggle is real\" is a common expression used by Millennials to express the challenges and difficulties they face in their daily lives, such as finding a job, paying off student loans, and navigating the complexities of adulthood.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model with few shot prompt\n",
    "prompt = \"\"\"Sentence: 'Life is pretty groovy.'\n",
    "Generation: Boomer\n",
    "\n",
    "Sentence: 'Life is all about adulting these days.'\n",
    "Generation: Millenial\n",
    "\n",
    "Sentence: 'My brother is such a simp.'\n",
    "Generation: Gen Z\n",
    "\n",
    "Sentence: 'The struggle is real.'\n",
    "Generation:\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the previous examples, the generation for the sentence \"I like to watch High school music during Christmas because the struggle is real.\" would be Millenial.\n",
      "\n",
      "The sentence \"The struggle is real\" was identified as Millenial, and the phrase \"the struggle is real\" is used again in this new sentence, indicating a Millenial generational association.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model with few shot prompt\n",
    "prompt = \"\"\"Sentence: 'Life is pretty groovy.'\n",
    "Generation: Boomer\n",
    "\n",
    "Sentence: 'Life is all about adulting these days.'\n",
    "Generation: Millenial\n",
    "\n",
    "Sentence: 'My brother is such a simp.'\n",
    "Generation: Gen Z\n",
    "\n",
    "Sentence: 'The struggle is real.'\n",
    "Generation: Millenial\n",
    "\n",
    "Sentence: 'I like to watch High school music during Christmas because the struggle is real.'\n",
    "Generation:\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context and language used in the sentence, the generation associated with the sentence \"I like to watch High school music during Christmas because the struggle is real\" would be Gen Z.\n",
      "\n",
      "The phrase \"High school music\" is likely a reference to the popular Disney Channel movie franchise \"High School Musical,\" which was particularly popular among younger audiences. Additionally, the use of the phrase \"the struggle is real\" is a common expression used by Gen Z to express relatable experiences or challenges.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the model with few shot prompt\n",
    "prompt = \"\"\"Sentence: 'Life is pretty groovy.'\n",
    "Generation: Boomer\n",
    "\n",
    "Sentence: 'Life is all about adulting these days.'\n",
    "Generation: Millenial\n",
    "\n",
    "Sentence: 'My brother is such a simp.'\n",
    "Generation: Gen Z\n",
    "\n",
    "Sentence: 'I like to watch High school music during Christmas because the struggle is real.'\n",
    "Generation:\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
