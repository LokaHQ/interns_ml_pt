{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeDE0BL93wqA"
      },
      "source": [
        "# 1. RAG with LangChain & Langfuse\n",
        "\n",
        "The idea of this notebook is to showcase how to utilize LLMOps tools like LangChain(for orchestration) and Langfuse(for monitoring & observation) to build a simple RAG system pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVJJUjAO3wqB"
      },
      "source": [
        "We need to download a PDF data which will be used as data for the RAG. For our RAG we are utilizing the paper:\n",
        "- Attention is All You Need, Vaswani et al. 2017\n",
        "- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al. 2018\n",
        "- Improving Language Understanding by Generative Pretraining, Radford et al. 2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DCqA2B-83wqB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-24 17:06:52--  https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
            "Resolving proceedings.neurips.cc (proceedings.neurips.cc)... 198.202.70.94\n",
            "Connecting to proceedings.neurips.cc (proceedings.neurips.cc)|198.202.70.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 569417 (556K) [application/pdf]\n",
            "Saving to: ‘attention_is_all_you_need.pdf’\n",
            "\n",
            "attention_is_all_yo 100%[===================>] 556,07K   666KB/s    in 0,8s    \n",
            "\n",
            "2024-10-24 17:06:54 (666 KB/s) - ‘attention_is_all_you_need.pdf’ saved [569417/569417]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the paper attention is all you need(the transformer paper) from the NeurIPS 2017 conference\n",
        "!wget https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf -O attention_is_all_you_need.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rrl0Ko0s3wqC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-24 17:06:54--  https://www.aclweb.org/anthology/N19-1423.pdf\n",
            "Resolving www.aclweb.org (www.aclweb.org)... 50.87.169.12\n",
            "Connecting to www.aclweb.org (www.aclweb.org)|50.87.169.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://aclanthology.org/N19-1423.pdf [following]\n",
            "--2024-10-24 17:06:55--  https://aclanthology.org/N19-1423.pdf\n",
            "Resolving aclanthology.org (aclanthology.org)... 174.138.37.75\n",
            "Connecting to aclanthology.org (aclanthology.org)|174.138.37.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 786279 (768K) [application/pdf]\n",
            "Saving to: ‘bert.pdf’\n",
            "\n",
            "bert.pdf            100%[===================>] 767,85K  1,28MB/s    in 0,6s    \n",
            "\n",
            "2024-10-24 17:06:56 (1,28 MB/s) - ‘bert.pdf’ saved [786279/786279]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding from the NAACL 2019 conference\n",
        "!wget https://www.aclweb.org/anthology/N19-1423.pdf -O bert.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mL3Y8rZU3wqC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-24 17:06:56--  https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
            "Resolving cdn.openai.com (cdn.openai.com)... 2620:1ec:bdf::42, 13.107.246.42\n",
            "Connecting to cdn.openai.com (cdn.openai.com)|2620:1ec:bdf::42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 541036 (528K) [application/pdf]\n",
            "Saving to: ‘gpt.pdf’\n",
            "\n",
            "gpt.pdf             100%[===================>] 528,36K  --.-KB/s    in 0,09s   \n",
            "\n",
            "2024-10-24 17:06:58 (5,76 MB/s) - ‘gpt.pdf’ saved [541036/541036]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the GPT paper\n",
        "!wget https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf -O gpt.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VPkF1LaA3wqC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (0.3.2)\n",
            "Requirement already satisfied: langchain-community in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (0.3.1)\n",
            "Requirement already satisfied: boto3 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (1.35.34)\n",
            "Requirement already satisfied: langfuse in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (2.52.2)\n",
            "Requirement already satisfied: faiss-cpu in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (1.9.0)\n",
            "Requirement already satisfied: PyMuPDF in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (1.24.11)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (3.10.9)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.8 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (0.3.12)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (0.1.131)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain-community) (2.5.2)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.34 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from boto3) (1.35.34)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from boto3) (0.10.2)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langfuse) (4.6.0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langfuse) (1.11.1)\n",
            "Requirement already satisfied: httpx<1.0,>=0.15.4 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langfuse) (0.27.2)\n",
            "Requirement already satisfied: idna<4.0,>=3.7 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langfuse) (3.10)\n",
            "Requirement already satisfied: packaging<25.0,>=23.2 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langfuse) (24.1)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.14 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langfuse) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from anyio<5.0.0,>=4.4.0->langfuse) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.34->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.34->boto3) (2.2.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.34->boto3) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/raquelcardoso/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the langchain and langchain-community packages\n",
        "!pip install langchain langchain-community boto3 langfuse faiss-cpu PyMuPDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGduBads3wqD"
      },
      "source": [
        "For our use case, we are going to work **Langchain** for orchestration, **Ragas** for evaluation and **Langfuse** for monitoring and observation.\n",
        "\n",
        "**Langchain** is a very powerful LLM/Agent orchestration tool that allows us to easily create and manage LLMs and Agents. **Langchain** provides all the necessary tools to create a whole RAG system, from the data ingestion pipeline to the inference pipeline.\n",
        "\n",
        "**Langfuse** is a tool that allows us to monitor and observe the performance of our LLMs. It provides a simple interface to monitor and observe the performance of our LLM applications by observing traces, latency, costs, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eSKIbHSF3wqD"
      },
      "outputs": [],
      "source": [
        "# Importing all the neccessary modules/libraries\n",
        "import os\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_aws import ChatBedrock\n",
        "from langchain_aws import BedrockEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langfuse.callback import CallbackHandler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKMId_OC3wqD"
      },
      "source": [
        "In this step, we need to define the configuration of the pipeline. In the configuration we are going to define the:\n",
        "- Region name and the credentials profile name of our AWS account\n",
        "- The public & secret key for our Languse server and the host url of the Langfuse server\n",
        "- The embedding model that we are going to use to embed the data and the configuration of the embedding model(dimension of the vector embeddings and the normalization of the embeddings)\n",
        "- The size of the documents chunks and the overlap between the chunks\n",
        "- The path of the PDF files that we are going to use to extract the data\n",
        "- The path of the vector database that we are going to use to save the embedded data\n",
        "- The LLM model that we are going to use to generate the output and the configuration of the LLM model(maximum tokens, top k, top p, temperature)\n",
        "- The retriever configuration, basically the metric and the number of documents that we are going to retrieve(other additional arguments if the metrics of the retriever is changed)\n",
        "- The input key, memory key and input variables that are going to be used in the prompt.\n",
        "- The prompt template used for the inference(chain) pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, dotenv_values\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oKYLUP1M3wqD"
      },
      "outputs": [],
      "source": [
        "# Defining the configuration\n",
        "REGION_NAME = \"us-west-2\"\n",
        "CREDENTIALS_PROFILE_NAME = \"ML\"\n",
        "\n",
        "PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
        "SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
        "HOST = os.getenv(\"LANGFUSE_HOST\")\n",
        "\n",
        "EMBEDDER_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
        "EMBEDDER_MODEL_KWARGS = {\n",
        "    \"dimensions\": 512,\n",
        "    \"normalize\": True\n",
        "}\n",
        "\n",
        "LLM_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\" # anthropic.claude-3-haiku-20240307-v1:0 or anthropic.claude-3-sonnet-20240229-v1:0 or anthropic.claude-v2:1\n",
        "LLM_MODEL_KWARGS = {\n",
        "    \"max_tokens\": 4096,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 1,\n",
        "    \"top_k\": 250,\n",
        "    \"stop_sequences\": [\"\\n\\nHuman\"]\n",
        "}\n",
        "\n",
        "CHUNK_SIZE = 2000\n",
        "CHUNK_OVERLAP = 100\n",
        "\n",
        "DATA_PATHS = [\n",
        "    \"attention_is_all_you_need.pdf\",\n",
        "    \"bert.pdf\",\n",
        "    \"gpt.pdf\"\n",
        "]\n",
        "\n",
        "VECTOR_STORE_PATH = \"./vector_database/\"\n",
        "\n",
        "SEARCH_TYPE = \"similarity\"\n",
        "RETRIEVER_KWARGS = {\n",
        "    \"k\": 5\n",
        "}\n",
        "\n",
        "VECTOR_STORE_PATH = \"./vector_database/\"\n",
        "\n",
        "INPUT_KEY = \"question\"\n",
        "MEMORY_KEY = \"history\"\n",
        "INPUT_VARIABLES = [\"context\", \"history\", \"question\"]\n",
        "\n",
        "# Inside in the prompt template, you can play with the system's persona, the context, the history, and the question.\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "System: You are a helpful, respectful and honest assistant for Machine Learning.\n",
        "Always answer as helpfully as possible, while being safe.\n",
        "Please ensure that your responses are socially unbiased and positive in nature.\n",
        "When addressing the user, always base your responses on the context provided and the previous chat history if its available.\n",
        "If you are unsure about the answer, please let the user know.\n",
        "If the user asks something that is not related to Machine Learning, please let the user know.\n",
        "Human:\n",
        "----------\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "----------\n",
        "<history>\n",
        "{history}\n",
        "</history>\n",
        "----------\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "----------\n",
        "Assistant:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu7o41vr3wqE"
      },
      "source": [
        "\n",
        "We need to define the chunker.\n",
        "\n",
        "**The chunker** is responsible for splitting the data into the desired chunks. In this case, we are going to split the data into chunks of 2000 tokens with an overlap of 100 tokens.\n",
        "\n",
        "The idea of why we are using larger chunks is to keep all the information of the document in the same chunk, so we can have a better representation of the document and the information that it contains.\n",
        "\n",
        "The overlap is used to keep track of the context between the chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e1niw-su3wqE"
      },
      "outputs": [],
      "source": [
        "# Defining the chunker\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "chunk_size=CHUNK_SIZE,\n",
        "chunk_overlap=CHUNK_OVERLAP\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTQcvdi43wqE"
      },
      "source": [
        "\n",
        "We need to load the data from the PDF files by parsing the data and splitting it into the desired chunks. We are going to use the chunker that we defined in the previous step to split the data into chunks.\n",
        "\n",
        "For loading the data we are going to use **PyMUPDFLoader** which is an excellent parser, that keeps the structure of the pdf document and allows us to extract the information of the docs in a very structured way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CfeN3cZ73wqE"
      },
      "outputs": [],
      "source": [
        "# Creating chunks from the documents\n",
        "global_chunks = []\n",
        "for data_path in DATA_PATHS:\n",
        "    loader = PyMuPDFLoader(os.path.join(os.getcwd(), data_path))\n",
        "    docs = loader.load()\n",
        "    chunks = splitter.split_documents(docs)\n",
        "    global_chunks.extend(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyrQnUa83wqE"
      },
      "source": [
        "We utilize the embedding model to embed the data.\n",
        "\n",
        "We are going to use the newest **amazon-titan embeddings model v2**, which is a very powerful model that can embed the data in a very low or high dimension, depending on the desired configuration.\n",
        "\n",
        "We are going to use the 512 dimension embeddings with the normalization of the embeddings, which is a standard configuration for the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wFcLmjdw3wqE"
      },
      "outputs": [],
      "source": [
        "# Creating the embedder\n",
        "embedder = BedrockEmbeddings(\n",
        "    model_id=EMBEDDER_MODEL_ID,\n",
        "    model_kwargs=EMBEDDER_MODEL_KWARGS,\n",
        "    region_name=REGION_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJAFcv_l3wqE"
      },
      "source": [
        "Now we are going to embed the data using the embedding model that we defined in the previous step and save the embedded data in the vector database. We are going to use the **FAISS** to save the embedded data in the vector database. **FAISS** is a very powerful vector database that can save the embedded data in a very efficient way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jrIKZt1i3wqE"
      },
      "outputs": [],
      "source": [
        "# Creating the vector store\n",
        "vector_store = FAISS.from_documents(documents=chunks, embedding=embedder)\n",
        "vector_store.save_local(VECTOR_STORE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkcamghl3wqF"
      },
      "source": [
        "\n",
        "We define the LLM model that we are going to use in the pipeline.\n",
        "\n",
        "For the LLM we are utilizing the most powerful Antropic model designed for systems like ours, that's the **Claude 3 Sonnet model**. This model is also very cheap to run and has excellent performance. The model has 200k context size window and is very powerful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Tz6pTvjv3wqF"
      },
      "outputs": [],
      "source": [
        "# Creating the LLM and Embedder models\n",
        "llm = ChatBedrock(region_name=REGION_NAME, credentials_profile_name=CREDENTIALS_PROFILE_NAME,model_id=LLM_MODEL_ID, model_kwargs=LLM_MODEL_KWARGS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJG-8vXi3wqF"
      },
      "source": [
        "\n",
        "We are going to load the vector database to be used for the **retriever**.\n",
        "\n",
        "**The retriever** is going to be used to retrieve the most similar documents to the input query. The search type is going to be similarity(cosine) and we are going to retrieve the top 5 documents. Some other approaches are changing the search type to mmr or similarity search with a threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Jkp5ZVeh3wqF"
      },
      "outputs": [],
      "source": [
        "# Loading the vector store and creating retriever\n",
        "vector_store = FAISS.load_local(VECTOR_STORE_PATH, embeddings=embedder, allow_dangerous_deserialization=True)\n",
        "retriever = vector_store.as_retriever(search_type=SEARCH_TYPE, **RETRIEVER_KWARGS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-Wpk0bf3wqF"
      },
      "source": [
        "\n",
        "We want to define the **conversational memory buffer** which will store the previous question and the history of the conversation(for that question).\n",
        "\n",
        "Also we are defining the **prompt template** that is going to be used in the pipeline. **The prompt template** is going to be used to generate the input for the LLM model. In the prompt template the context, history and the question is going to be passed + addional system prompt which can be changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7v-5_0TJ3wqF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/w0/r0k6chkn2nzd_g1c2pdl2pf40000gn/T/ipykernel_76672/3752146924.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(memory_key=MEMORY_KEY, input_key=INPUT_KEY, k=3, ai_prefix=\"Assistant\")\n"
          ]
        }
      ],
      "source": [
        "# Creating the memory and the prompt template\n",
        "memory = ConversationBufferWindowMemory(memory_key=MEMORY_KEY, input_key=INPUT_KEY, k=3, ai_prefix=\"Assistant\")\n",
        "prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=INPUT_VARIABLES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB5fAEkc3wqF"
      },
      "source": [
        "Before invoking the pipeline, we need to define the **Langfuse** handler which is going to be used to monitor and observe the performance of the pipeline.\n",
        "We are creating a langfuse callback handler that is going to direct all the inputs and outputs from the system to the Langfuse server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sWPqnZmK3wqF"
      },
      "outputs": [],
      "source": [
        "# Creating the callback handler\n",
        "langfuse_callback= CallbackHandler(\n",
        "        public_key=PUBLIC_KEY,\n",
        "        secret_key=SECRET_KEY,\n",
        "        host=HOST,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIbeGXTY3wqF"
      },
      "source": [
        "\n",
        "In this step we are going to define **the chain** that is going to be used in the pipeline.\n",
        "**The chain** is going to be composed of:\n",
        "- retriever,\n",
        "- LLM model,\n",
        "- conversational memory buffer\n",
        "- prompt template\n",
        "- Langfuse handler.\n",
        "  \n",
        "The chain is going to be used to generate the output for the input query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Z1XF27Sb3wqF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Creating the Chain for usage\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            verbose=True,\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\n",
        "                \"prompt\": prompt,\n",
        "                \"memory\": memory\n",
        "            }\n",
        ")\n",
        "response = chain.invoke(\"What is attention mechanism?\", config={\"callbacks\": [langfuse_callback]})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
