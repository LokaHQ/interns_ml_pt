{"cells":[{"cell_type":"markdown","metadata":{"id":"ZuY70VpQnONo"},"source":["# 1. Data Loading, Splitting and Ingestion Pipeline\n","\n","The idea of this notebook is to show the first steps of the pipeline, which is the data ingestion pipeline.\n","The data ingestion pipeline is responsible for:\n","- loading(parsing) the data from the source(locally available PDF data or data on S3)\n","- splitting the data into the desired chunks\n","- embedding the data\n","- saving it in a vector database.\n","\n","<b>The whole pipeline (Data Loading, Splitting and Ingestion Pipeline is circled with red-1)</b>\n","\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"XXiIVt3EnONq"},"source":["We need to download a PDF data which will be used as data for the RAG. For our RAG we are utilizing the paper:\n","- Attention is All You Need, Vaswani et al. 2017\n","- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al. 2018\n","- Improving Language Understanding by Generative Pretraining, Radford et al. 2018"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yTsO_c16nONq"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-10-02 13:16:07--  https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n","Resolving proceedings.neurips.cc (proceedings.neurips.cc)... 198.202.70.94\n","Connecting to proceedings.neurips.cc (proceedings.neurips.cc)|198.202.70.94|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 569417 (556K) [application/pdf]\n","Saving to: ‘attention_is_all_you_need.pdf’\n","\n","attention_is_all_yo 100%[===================>] 556,07K   588KB/s    in 0,9s    \n","\n","2024-10-02 13:16:09 (588 KB/s) - ‘attention_is_all_you_need.pdf’ saved [569417/569417]\n","\n"]}],"source":["# Download the paper attention is all you need(the transformer paper) from the NeurIPS 2017 conference\n","!wget https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf -O attention_is_all_you_need.pdf"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Aay8EzFGnONq"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-10-02 13:16:11--  https://www.aclweb.org/anthology/N19-1423.pdf\n","Resolving www.aclweb.org (www.aclweb.org)... 50.87.169.12\n","Connecting to www.aclweb.org (www.aclweb.org)|50.87.169.12|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://aclanthology.org/N19-1423.pdf [following]\n","--2024-10-02 13:16:12--  https://aclanthology.org/N19-1423.pdf\n","Resolving aclanthology.org (aclanthology.org)... 174.138.37.75\n","Connecting to aclanthology.org (aclanthology.org)|174.138.37.75|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 786279 (768K) [application/pdf]\n","Saving to: ‘bert.pdf’\n","\n","bert.pdf            100%[===================>] 767,85K   791KB/s    in 1,0s    \n","\n","2024-10-02 13:16:13 (791 KB/s) - ‘bert.pdf’ saved [786279/786279]\n","\n"]}],"source":["# Download the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding from the NAACL 2019 conference\n","!wget https://www.aclweb.org/anthology/N19-1423.pdf -O bert.pdf"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ocP5daTZnONq"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-10-02 13:16:14--  https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n","Resolving cdn.openai.com (cdn.openai.com)... 2620:1ec:bdf::42, 13.107.246.42\n","Connecting to cdn.openai.com (cdn.openai.com)|2620:1ec:bdf::42|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 541036 (528K) [application/pdf]\n","Saving to: ‘gpt.pdf’\n","\n","gpt.pdf             100%[===================>] 528,36K  1,41MB/s    in 0,4s    \n","\n","2024-10-02 13:16:15 (1,41 MB/s) - ‘gpt.pdf’ saved [541036/541036]\n","\n"]}],"source":["# Download the GPT paper\n","!wget https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf -O gpt.pdf"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"MZ43ECYSnONr"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: langchain in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (0.3.1)\n","Requirement already satisfied: langchain-community in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (0.3.1)\n","Requirement already satisfied: boto3 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (1.35.31)\n","Requirement already satisfied: PyYAML>=5.3 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (3.10.8)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (0.3.7)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (0.3.0)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (0.1.129)\n","Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-community) (2.5.2)\n","Requirement already satisfied: botocore<1.36.0,>=1.35.31 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from boto3) (1.35.31)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from boto3) (1.0.1)\n","Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from boto3) (0.10.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.31->boto3) (2.9.0.post0)\n","Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.31->boto3) (2.2.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n","Requirement already satisfied: annotated-types>=0.6.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n","Requirement already satisfied: anyio in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.0)\n","Requirement already satisfied: httpcore==1.* in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n","Requirement already satisfied: sniffio in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain) (3.0.0)\n","Requirement already satisfied: six>=1.5 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.31->boto3) (1.16.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"]}],"source":["# Install the langchain and langchain-community packages\n","!pip install langchain langchain-community boto3"]},{"cell_type":"markdown","metadata":{"id":"edUTWmeRnONr"},"source":["For our use case, we are going to work **Langchain**.\n","\n","**Langchain** is a very powerful LLM/Agent orchestration tool that allows us to easily create and manage LLMs and Agents. **Langchain** provides all the necessary tools to create a whole RAG system, from the data ingestion pipeline to the inference pipeline."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hOqN-3bhnONr"},"outputs":[],"source":["# Importing all the neccessary modules/libraries\n","import os\n","\n","from langchain.document_loaders import PyMuPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.embeddings import BedrockEmbeddings\n","from langchain_community.vectorstores import FAISS"]},{"cell_type":"markdown","metadata":{"id":"Hgiw6SVhnONr"},"source":["In this step, we need to define the configuration of the first step of the pipeline. In the configuration we are going to define the:\n","- Region name and the credentials profile name of our AWS account\n","- The embedding model that we are going to use to embed the data and the configuration of the embedding model(dimension of the vector embeddings and the normalization of the embeddings)\n","- The size of the documents chunks and the overlap between the chunks\n","- The path of the PDF files that we are going to use to extract the data\n","- The path of the vector database that we are going to use to save the embedded data"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"T3JoIlqJnONr"},"outputs":[],"source":["# Defining the configuration\n","REGION_NAME = \"us-west-2\"\n","#CREDENTIALS_PROFILE_NAME = \"ML\"\n","EMBEDDER_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n","EMBEDDER_MODEL_KWARGS = {\n","    \"dimensions\": 512,\n","    \"normalize\": True\n","}\n","\n","CHUNK_SIZE = 2000\n","CHUNK_OVERLAP = 100\n","\n","DATA_PATHS = [\n","    \"attention_is_all_you_need.pdf\",\n","    \"bert.pdf\",\n","    \"gpt.pdf\"\n","]\n","\n","VECTOR_STORE_PATH = \"./vector_database/\""]},{"cell_type":"markdown","metadata":{"id":"b4d1_bRMnONr"},"source":["\n","We need to define the chunker.\n","\n","**The chunker** is responsible for splitting the data into the desired chunks. In this case, we are going to split the data into chunks of 2000 tokens with an overlap of 100 tokens.\n","\n","The idea of why we are using larger chunks is to keep all the information of the document in the same chunk, so we can have a better representation of the document and the information that it contains.\n","\n","The overlap is used to keep track of the context between the chunks."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"S-F5IxMxnONr"},"outputs":[],"source":["# Defining the chunker\n","splitter = RecursiveCharacterTextSplitter(\n","chunk_size=CHUNK_SIZE,\n","chunk_overlap=CHUNK_OVERLAP\n",")"]},{"cell_type":"markdown","metadata":{"id":"b3Ag38ZinONr"},"source":["\n","We need to load the data from the PDF files by parsing the data and splitting it into the desired chunks. We are going to use the chunker that we defined in the previous step to split the data into chunks.\n","\n","For loading the data we are going to use **PyMUPDFLoader** which is an excellent parser, that keeps the structure of the pdf document and allows us to extract the information of the docs in a very structured way."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"93TzLP_tnONs"},"outputs":[],"source":["# Creating chunks from the documents\n","global_chunks = []\n","for data_path in DATA_PATHS:\n","    loader = PyMuPDFLoader(os.path.join(os.getcwd(), data_path))\n","    docs = loader.load()\n","    chunks = splitter.split_documents(docs)\n","    global_chunks.extend(chunks)"]},{"cell_type":"markdown","metadata":{"id":"JTs36gInnONs"},"source":["We utilize the embedding model to embed the data.\n","\n","We are going to use the newest **amazon-titan embeddings model v2**, which is a very powerful model that can embed the data in a very low or high dimension, depending on the desired configuration.\n","\n","We are going to use the 512 dimension embeddings with the normalization of the embeddings, which is a standard configuration for the embeddings."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"nwuBCRJOnONs"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/w0/r0k6chkn2nzd_g1c2pdl2pf40000gn/T/ipykernel_7879/592713195.py:2: LangChainDeprecationWarning: The class `BedrockEmbeddings` was deprecated in LangChain 0.2.11 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-aws package and should be used instead. To use it run `pip install -U :class:`~langchain-aws` and import as `from :class:`~langchain_aws import BedrockEmbeddings``.\n","  embedder = BedrockEmbeddings(\n"]}],"source":["# Creating the embedder\n","embedder = BedrockEmbeddings(\n","    model_id=EMBEDDER_MODEL_ID,\n","    model_kwargs=EMBEDDER_MODEL_KWARGS,\n","    region_name=REGION_NAME,\n","    #credentials_profile_name=CREDENTIALS_PROFILE_NAME\n",")"]},{"cell_type":"markdown","metadata":{"id":"rPcRucF6nONs"},"source":["Now we are going to embed the data using the embedding model that we defined in the previous step and save the embedded data in the vector database. We are going to use the **FAISS** to save the embedded data in the vector database. **FAISS** is a very powerful vector database that can save the embedded data in a very efficient way."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"cKmMtRiNnONs"},"outputs":[],"source":["# Creating the vector store\n","vector_store = FAISS.from_documents(documents=chunks, embedding=embedder)\n","vector_store.save_local(VECTOR_STORE_PATH)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"MLOps","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}
