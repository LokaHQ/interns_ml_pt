{"cells":[{"cell_type":"markdown","metadata":{"id":"0FHPChGJmIlv"},"source":["# 2. RAG system - Retriever, Generator pipeline\n","\n","The idea of this notebook is to show how to configure the RAG system to make an inference pipeline.\n","The pipeline is composed first by:\n","- defining all the configuration needed\n","- defining the LLM and the embeddings models\n","- loading the ingested vector database and defining the retrieval from it\n","- defining the prompt template and the conversational memory buffer,\n","- finally defining the chain(retrieval QA chain).\n","\n","<b>The whole pipeline (RAG system - Retriever, Generator pipeline is circled with blue-2)</b>\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"gdCdkgzOmIlx"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: langchain in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (0.3.1)\n","Requirement already satisfied: langchain-community in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (0.3.1)\n","Requirement already satisfied: boto3 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (1.35.31)\n","Requirement already satisfied: PyYAML>=5.3 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (3.10.8)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (0.3.7)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (0.3.0)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (0.1.129)\n","Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-community) (2.5.2)\n","Requirement already satisfied: botocore<1.36.0,>=1.35.31 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from boto3) (1.35.31)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from boto3) (1.0.1)\n","Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from boto3) (0.10.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.31->boto3) (2.9.0.post0)\n","Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.31->boto3) (2.2.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n","Requirement already satisfied: annotated-types>=0.6.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n","Requirement already satisfied: anyio in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.0)\n","Requirement already satisfied: httpcore==1.* in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n","Requirement already satisfied: sniffio in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain) (3.0.0)\n","Requirement already satisfied: six>=1.5 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.31->boto3) (1.16.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/raquelcardoso/myenv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"]}],"source":["# Install the langchain and langchain-community packages\n","!pip install langchain langchain-community boto3"]},{"cell_type":"markdown","metadata":{"id":"LcnkBUbomIly"},"source":["For our use case, we are going to work **Langchain**.\n","\n","**Langchain** is a very powerful LLM/Agent orchestration tool that allows us to easily create and manage LLMs and Agents. **Langchain** provides all the necessary tools to create a whole RAG system, from the data ingestion pipeline to the inference pipeline."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"_Lb6D88lmIly"},"outputs":[],"source":["# Importing all the neccessary modules/libraries\n","from langchain.chains import RetrievalQA\n","from langchain.memory import ConversationBufferWindowMemory\n","from langchain.prompts import PromptTemplate\n","#from langchain_community.chat_models import BedrockChat\n","from langchain_aws import ChatBedrock\n","from langchain_aws import BedrockEmbeddings\n","#from langchain_community.embeddings import BedrockEmbeddings\n","from langchain_community.vectorstores import FAISS"]},{"cell_type":"markdown","metadata":{"id":"JCxzfnVemIlz"},"source":["In this step, we need to define the configuration of the first step of the pipeline. In the configuration we are going to define the:\n","- Region name and the credentials profile name of our AWS account\n","- The embedding model that we are going to use to embed the data and the configuration of the embedding model(dimension of the vector embeddings and the normalization of the embeddings)\n","- The LLM model that we are going to use to generate the output and the configuration of the LLM model(maximum tokens, top k, top p, temperature)\n","- The retriever configuration, basically the metric and the number of documents that we are going to retrieve(other additional arguments if the metrics of the retriever is changed)\n","- The path of the vector database that we are going to use for the retrieval\n","- The input key, memory key and input variables that are going to be used in the prompt.\n","- The prompt template used for the inference(chain) pipeline"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"uxxqn-f6mIlz"},"outputs":[],"source":["# Defining the configuration\n","REGION_NAME = \"us-east-1\"\n","#CREDENTIALS_PROFILE_NAME = \"ML\"\n","\n","EMBEDDER_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n","EMBEDDER_MODEL_KWARGS = {\n","    \"dimensions\": 512,\n","    \"normalize\": True\n","}\n","\n","LLM_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\" # anthropic.claude-3-haiku-20240307-v1:0 or anthropic.claude-3-sonnet-20240229-v1:0 or anthropic.claude-v2:1\n","LLM_MODEL_KWARGS = {\n","    \"max_tokens\": 4096,\n","    \"temperature\": 0.1,\n","    \"top_p\": 1,\n","    \"top_k\": 250,\n","    \"stop_sequences\": [\"\\n\\nHuman\"]\n","}\n","\n","SEARCH_TYPE = \"similarity\"\n","RETRIEVER_KWARGS = {\n","    \"k\": 5\n","}\n","\n","VECTOR_STORE_PATH = \"./vector_database/\"\n","\n","INPUT_KEY = \"question\"\n","MEMORY_KEY = \"history\"\n","INPUT_VARIABLES = [\"context\", \"history\", \"question\"]\n","\n","# Inside in the prompt template, you can play with the system's persona, the context, the history, and the question.\n","PROMPT_TEMPLATE = \"\"\"\n","System: You are a helpful, respectful and honest assistant for Machine Learning.\n","Always answer as helpfully as possible, while being safe.\n","Please ensure that your responses are socially unbiased and positive in nature.\n","When addressing the user, always base your responses on the context provided and the previous chat history if its available.\n","If you are unsure about the answer, please let the user know.\n","If the user asks something that is not related to Machine Learning, please let the user know.\n","Human:\n","----------\n","<context>\n","{context}\n","</context>\n","----------\n","<history>\n","{history}\n","</history>\n","----------\n","<question>\n","{question}\n","</question>\n","----------\n","Assistant:\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"vZEMRHS-mIlz"},"source":["\n","We define the LLM model and the embeddings model that we are going to use in the pipeline.\n","\n","For the LLM we are utilizing the most powerful Antropic model designed for systems like ours, that's the **Claude 3 Sonnet model**. This model is also very cheap to run and has excellent performance. The model has 200k context size window and is very powerful.\n","\n","For the embeddings model we are going to work with with the same embedding model that we used in the data loading splitting ingestion pipeline."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"HFARd1BdmIl0"},"outputs":[],"source":["# Creating the LLM and Embedder models\n","llm = ChatBedrock(region_name=REGION_NAME,model_id=LLM_MODEL_ID, model_kwargs=LLM_MODEL_KWARGS)\n","embedder = BedrockEmbeddings(region_name=REGION_NAME, model_id=EMBEDDER_MODEL_ID, model_kwargs=EMBEDDER_MODEL_KWARGS)"]},{"cell_type":"markdown","metadata":{"id":"W187HElkmIl0"},"source":["\n","We are going to load the vector database that we used in the data ingestion pipeline and define the retriever that we are going to use in this pipeline.\n","\n","**The retriever** is going to be used to retrieve the most similar documents to the input query. The search type is going to be similarity(cosine) and we are going to retrieve the top 5 documents. Some other approaches are changing the search type to mmr or similarity search with a threshold."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"mvdWNunnmIl0"},"outputs":[],"source":["# Loading the vector store and creating retriever\n","vector_store = FAISS.load_local(VECTOR_STORE_PATH, embeddings=embedder, allow_dangerous_deserialization=True)\n","retriever = vector_store.as_retriever(search_type=SEARCH_TYPE, **RETRIEVER_KWARGS)"]},{"cell_type":"markdown","metadata":{"id":"eRCHzVFQmIl0"},"source":["\n","We want to define the **conversational memory buffer** which will store the previous question and the history of the conversation(for that question).\n","\n","Also we are defining the **prompt template** that is going to be used in the pipeline. **The prompt template** is going to be used to generate the input for the LLM model. In the prompt template the context, history and the question is going to be passed + addional system prompt which can be changed."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"QoMJaFQ0mIl0"},"outputs":[],"source":["# Creating the memory and the prompt template\n","memory = ConversationBufferWindowMemory(memory_key=MEMORY_KEY, input_key=INPUT_KEY, k=3, ai_prefix=\"Assistant\")\n","prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=INPUT_VARIABLES)"]},{"cell_type":"markdown","metadata":{"id":"u9zrixkfmIl0"},"source":["\n","In this step we are going to define **the chain** that is going to be used in the pipeline.\n","**The chain** is going to be composed of:\n","- retriever,\n","- LLM model,\n","- conversational memory buffer\n","- prompt template.\n","  \n","The chain is going to be used to generate the output for the input query."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"iS-E8xABmIl1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# Creating the Chain for usage\n","chain = RetrievalQA.from_chain_type(\n","            llm=llm,\n","            retriever=retriever,\n","            verbose=True,\n","            return_source_documents=True,\n","            chain_type_kwargs={\n","                \"prompt\": prompt,\n","                \"memory\": memory\n","            }\n",")\n","response = chain.invoke(\"What is attention mechanism?\")"]},{"cell_type":"markdown","metadata":{"id":"ur8SS11pmIl1"},"source":["\n","Let's check the pipeline's response, what elements it has."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"lyzF9AzrmIl1"},"outputs":[{"data":{"text/plain":["{'query': 'What is attention mechanism?',\n"," 'result': 'The attention mechanism is a key component of transformer models that allows the model to focus on the relevant parts of the input sequence when making predictions for a specific part of the output sequence.\\n\\nIn traditional sequence-to-sequence models like RNNs and LSTMs, the entire input sequence gets encoded into a fixed-length vector representation from which the output sequence is decoded. This can make it difficult to capture long-range dependencies in the input.\\n\\nThe attention mechanism helps alleviate this by calculating an attention score for each part of the input sequence that indicates how relevant it is to the current step of the output sequence being generated. This allows the model to selectively focus on the most relevant pieces of information from the input when producing each part of the output.\\n\\nThere are different variants of attention like self-attention (used in transformers) where the attention scores are calculated over the input itself, and encoder-decoder attention where the decoder attends to the encoder representations of the input sequence.\\n\\nThe attention mechanism has been a key innovation that enabled transformer models to achieve state-of-the-art results on many natural language processing tasks by better capturing long-range dependencies and handling variable-length input/output sequences more effectively.',\n"," 'source_documents': [Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 4, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='Table 1: A list of the different tasks and datasets used in our experiments.\\nTask\\nDatasets\\nNatural language inference\\nSNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25]\\nQuestion Answering\\nRACE [30], Story Cloze [40]\\nSentence similarity\\nMSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6]\\nClassiﬁcation\\nStanford Sentiment Treebank-2 [54], CoLA [65]\\nbut is shufﬂed at a sentence level - destroying long-range structure. Our language model achieves a\\nvery low token level perplexity of 18.4 on this corpus.\\nModel speciﬁcations\\nOur model largely follows the original transformer work [62]. We trained a\\n12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12\\nattention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.\\nWe used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate\\nwas increased linearly from zero over the ﬁrst 2000 updates and annealed to 0 using a cosine schedule.\\nWe train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.\\nSince layernorm [2] is used extensively throughout the model, a simple weight initialization of\\nN(0, 0.02) was sufﬁcient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53]\\nand residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also\\nemployed a modiﬁed version of L2 regularization proposed in [37], with w = 0.01 on all non bias or\\ngain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]. We\\nused learned position embeddings instead of the sinusoidal version proposed in the original work.\\nWe use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and\\nwhitespace, and use the spaCy tokenizer.3\\nFine-tuning details\\nUnless speciﬁed, we reuse the hyperparameter settings from unsupervised'),\n","  Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 4, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='sentences, and handle aspects of linguistic ambiguity. On RTE, one of the smaller datasets we\\nevaluate on (2490 examples), we achieve an accuracy of 56%, which is below the 61.7% reported by a\\nmulti-task biLSTM model. Given the strong performance of our approach on larger NLI datasets, it is\\nlikely our model will beneﬁt from multi-task training as well but we have not explored this currently.\\n2https://ftfy.readthedocs.io/en/latest/\\n3https://spacy.io/\\n5'),\n","  Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 6, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='a random guess baseline and the current state-of-the-art with a single model.\\nZero-shot Behaviors\\nWe’d like to better understand why language model pre-training of transform-\\ners is effective. A hypothesis is that the underlying generative model learns to perform many of the\\ntasks we evaluate on in order to improve its language modeling capability and that the more structured\\n7'),\n","  Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 7, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='Table 5: Analysis of various model ablations on different tasks. Avg. score is a unweighted average\\nof all the results. (mc= Mathews correlation, acc=Accuracy, pc=Pearson correlation)\\nMethod\\nAvg. Score\\nCoLA\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nRTE\\n(mc)\\n(acc)\\n(F1)\\n(pc)\\n(F1)\\n(acc)\\n(acc)\\n(acc)\\nTransformer w/ aux LM (full)\\n74.7\\n45.4\\n91.3\\n82.3\\n82.0\\n70.3\\n81.8\\n88.1\\n56.0\\nTransformer w/o pre-training\\n59.9\\n18.9\\n84.0\\n79.4\\n30.9\\n65.5\\n75.7\\n71.2\\n53.8\\nTransformer w/o aux LM\\n75.0\\n47.9\\n92.0\\n84.9\\n83.2\\n69.8\\n81.1\\n86.9\\n54.4\\nLSTM w/ aux LM\\n69.1\\n30.3\\n90.5\\n83.2\\n71.8\\n68.1\\n73.7\\n81.1\\n54.6\\nattentional memory of the transformer assists in transfer compared to LSTMs. We designed a series\\nof heuristic solutions that use the underlying generative model to perform tasks without supervised\\nﬁnetuning. We visualize the effectiveness of these heuristic solutions over the course of generative\\npre-training in Fig 2(right). We observe the performance of these heuristics is stable and steadily\\nincreases over training suggesting that generative pretraining supports the learning of a wide variety\\nof task relevant functionality. We also observe the LSTM exhibits higher variance in its zero-shot\\nperformance suggesting that the inductive bias of the Transformer architecture assists in transfer.\\nFor CoLA (linguistic acceptability), examples are scored as the average token log-probability the\\ngenerative model assigns and predictions are made by thresholding. For SST-2 (sentiment analysis),\\nwe append the token very to each example and restrict the language model’s output distribution to only\\nthe words positive and negative and guess the token it assigns higher probability to as the prediction.\\nFor RACE (question answering), we pick the answer the generative model assigns the highest average\\ntoken log-probability when conditioned on the document and question. For DPRD [46] (winograd\\nschemas), we replace the deﬁnite pronoun with the two possible referrents and predict the resolution')]}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Checking the response\n","response"]},{"cell_type":"markdown","metadata":{"id":"kjlAiXhLmIl1"},"source":["\n","In this step we are going to check the result of the pipeline, the pure output answer + the documents(content) that were retrieved from the vector database."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"r5tkJcK8mIl1"},"outputs":[{"name":"stdout","output_type":"stream","text":["The attention mechanism is a key component of transformer models that allows the model to focus on the relevant parts of the input sequence when making predictions for a specific part of the output sequence.\n","\n","In traditional sequence-to-sequence models like RNNs and LSTMs, the entire input sequence gets encoded into a fixed-length vector representation from which the output sequence is decoded. This can make it difficult to capture long-range dependencies in the input.\n","\n","The attention mechanism helps alleviate this by calculating an attention score for each part of the input sequence that indicates how relevant it is to the current step of the output sequence being generated. This allows the model to selectively focus on the most relevant pieces of information from the input when producing each part of the output.\n","\n","There are different variants of attention like self-attention (used in transformers) where the attention scores are calculated over the input itself, and encoder-decoder attention where the decoder attends to the encoder representations of the input sequence.\n","\n","The attention mechanism has been a key innovation that enabled transformer models to achieve state-of-the-art results on many natural language processing tasks by better capturing long-range dependencies and handling variable-length input/output sequences more effectively.\n"]}],"source":["# Printing the answer\n","print(response[\"result\"])"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"wPE-wkDMmIl1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","################################################# Document #################################################\n","Table 1: A list of the different tasks and datasets used in our experiments.\n","Task\n","Datasets\n","Natural language inference\n","SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25]\n","Question Answering\n","RACE [30], Story Cloze [40]\n","Sentence similarity\n","MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6]\n","Classiﬁcation\n","Stanford Sentiment Treebank-2 [54], CoLA [65]\n","but is shufﬂed at a sentence level - destroying long-range structure. Our language model achieves a\n","very low token level perplexity of 18.4 on this corpus.\n","Model speciﬁcations\n","Our model largely follows the original transformer work [62]. We trained a\n","12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12\n","attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.\n","We used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate\n","was increased linearly from zero over the ﬁrst 2000 updates and annealed to 0 using a cosine schedule.\n","We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.\n","Since layernorm [2] is used extensively throughout the model, a simple weight initialization of\n","N(0, 0.02) was sufﬁcient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53]\n","and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also\n","employed a modiﬁed version of L2 regularization proposed in [37], with w = 0.01 on all non bias or\n","gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]. We\n","used learned position embeddings instead of the sinusoidal version proposed in the original work.\n","We use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and\n","whitespace, and use the spaCy tokenizer.3\n","Fine-tuning details\n","Unless speciﬁed, we reuse the hyperparameter settings from unsupervised\n","{'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 4, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}\n","\n","################################################# Document #################################################\n","sentences, and handle aspects of linguistic ambiguity. On RTE, one of the smaller datasets we\n","evaluate on (2490 examples), we achieve an accuracy of 56%, which is below the 61.7% reported by a\n","multi-task biLSTM model. Given the strong performance of our approach on larger NLI datasets, it is\n","likely our model will beneﬁt from multi-task training as well but we have not explored this currently.\n","2https://ftfy.readthedocs.io/en/latest/\n","3https://spacy.io/\n","5\n","{'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 4, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}\n","\n","################################################# Document #################################################\n","a random guess baseline and the current state-of-the-art with a single model.\n","Zero-shot Behaviors\n","We’d like to better understand why language model pre-training of transform-\n","ers is effective. A hypothesis is that the underlying generative model learns to perform many of the\n","tasks we evaluate on in order to improve its language modeling capability and that the more structured\n","7\n","{'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 6, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}\n","\n","################################################# Document #################################################\n","Table 5: Analysis of various model ablations on different tasks. Avg. score is a unweighted average\n","of all the results. (mc= Mathews correlation, acc=Accuracy, pc=Pearson correlation)\n","Method\n","Avg. Score\n","CoLA\n","SST2\n","MRPC\n","STSB\n","QQP\n","MNLI\n","QNLI\n","RTE\n","(mc)\n","(acc)\n","(F1)\n","(pc)\n","(F1)\n","(acc)\n","(acc)\n","(acc)\n","Transformer w/ aux LM (full)\n","74.7\n","45.4\n","91.3\n","82.3\n","82.0\n","70.3\n","81.8\n","88.1\n","56.0\n","Transformer w/o pre-training\n","59.9\n","18.9\n","84.0\n","79.4\n","30.9\n","65.5\n","75.7\n","71.2\n","53.8\n","Transformer w/o aux LM\n","75.0\n","47.9\n","92.0\n","84.9\n","83.2\n","69.8\n","81.1\n","86.9\n","54.4\n","LSTM w/ aux LM\n","69.1\n","30.3\n","90.5\n","83.2\n","71.8\n","68.1\n","73.7\n","81.1\n","54.6\n","attentional memory of the transformer assists in transfer compared to LSTMs. We designed a series\n","of heuristic solutions that use the underlying generative model to perform tasks without supervised\n","ﬁnetuning. We visualize the effectiveness of these heuristic solutions over the course of generative\n","pre-training in Fig 2(right). We observe the performance of these heuristics is stable and steadily\n","increases over training suggesting that generative pretraining supports the learning of a wide variety\n","of task relevant functionality. We also observe the LSTM exhibits higher variance in its zero-shot\n","performance suggesting that the inductive bias of the Transformer architecture assists in transfer.\n","For CoLA (linguistic acceptability), examples are scored as the average token log-probability the\n","generative model assigns and predictions are made by thresholding. For SST-2 (sentiment analysis),\n","we append the token very to each example and restrict the language model’s output distribution to only\n","the words positive and negative and guess the token it assigns higher probability to as the prediction.\n","For RACE (question answering), we pick the answer the generative model assigns the highest average\n","token log-probability when conditioned on the document and question. For DPRD [46] (winograd\n","schemas), we replace the deﬁnite pronoun with the two possible referrents and predict the resolution\n","{'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 7, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}\n"]}],"source":["# Getting the documents used\n","for doc in response[\"source_documents\"]:\n","    print(\"\\n################################################# Document #################################################\")\n","    print(doc.page_content)\n","    print(doc.metadata)"]},{"cell_type":"markdown","metadata":{"id":"y8pjYU1vmIl1"},"source":["\n","Let's test a full conversation with the pipeline. We are going to ask a question or give an input and the pipeline is going to generate the answer. Then we are going to ask another question and the pipeline is going to generate the answer based on the previous question and the history of the conversation."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"7qVmYvydmIl1"},"outputs":[],"source":["# Resetting the chain\n","memory = ConversationBufferWindowMemory(memory_key=MEMORY_KEY, input_key=INPUT_KEY, k=3, ai_prefix=\"Assistant\")\n","chain = RetrievalQA.from_chain_type(\n","            llm=llm,\n","            retriever=retriever,\n","            verbose=True,\n","            return_source_documents=True,\n","            chain_type_kwargs={\n","                \"prompt\": prompt,\n","                \"memory\": memory\n","            }\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wB8YZ45MmIl1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","The BERT (Bidirectional Encoder Representations from Transformers) model involves two main steps:\n","\n","1. Pre-training: In this step, the BERT model is pre-trained on a large corpus of unlabeled text using two unsupervised tasks:\n","\n","- Masked Language Modeling (MLM): Here, some tokens in the input sequence are randomly masked, and the model learns to predict the masked tokens based on the context.\n","\n","- Next Sentence Prediction (NSP): The model receives pairs of sentences and learns to predict whether the second sentence follows the first in the original text.\n","\n","This pre-training allows BERT to develop deep bidirectional representations by jointly conditioning on both left and right context in all layers.\n","\n","2. Fine-tuning: After pre-training, the BERT model is fine-tuned on labeled data from the downstream task of interest, such as text classification, question answering, or natural language inference. During fine-tuning, the pre-trained BERT model is treated as a feature extractor, and a small number of output layers are added and trained for the specific task.\n","\n","The key innovation of BERT is the bidirectional pre-training, which allows the model to learn rich representations that can be effectively transferred to various natural language processing tasks through fine-tuning on task-specific labeled data.\n"]}],"source":["# Asking a new question and printing the answer\n","response = chain.invoke(\"What are the two steps in BERT model?\")\n","print(response[\"result\"])"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"qnjrmOTJmIl1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 4, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='sentences, and handle aspects of linguistic ambiguity. On RTE, one of the smaller datasets we\\nevaluate on (2490 examples), we achieve an accuracy of 56%, which is below the 61.7% reported by a\\nmulti-task biLSTM model. Given the strong performance of our approach on larger NLI datasets, it is\\nlikely our model will beneﬁt from multi-task training as well but we have not explored this currently.\\n2https://ftfy.readthedocs.io/en/latest/\\n3https://spacy.io/\\n5'), Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 7, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='Table 5: Analysis of various model ablations on different tasks. Avg. score is a unweighted average\\nof all the results. (mc= Mathews correlation, acc=Accuracy, pc=Pearson correlation)\\nMethod\\nAvg. Score\\nCoLA\\nSST2\\nMRPC\\nSTSB\\nQQP\\nMNLI\\nQNLI\\nRTE\\n(mc)\\n(acc)\\n(F1)\\n(pc)\\n(F1)\\n(acc)\\n(acc)\\n(acc)\\nTransformer w/ aux LM (full)\\n74.7\\n45.4\\n91.3\\n82.3\\n82.0\\n70.3\\n81.8\\n88.1\\n56.0\\nTransformer w/o pre-training\\n59.9\\n18.9\\n84.0\\n79.4\\n30.9\\n65.5\\n75.7\\n71.2\\n53.8\\nTransformer w/o aux LM\\n75.0\\n47.9\\n92.0\\n84.9\\n83.2\\n69.8\\n81.1\\n86.9\\n54.4\\nLSTM w/ aux LM\\n69.1\\n30.3\\n90.5\\n83.2\\n71.8\\n68.1\\n73.7\\n81.1\\n54.6\\nattentional memory of the transformer assists in transfer compared to LSTMs. We designed a series\\nof heuristic solutions that use the underlying generative model to perform tasks without supervised\\nﬁnetuning. We visualize the effectiveness of these heuristic solutions over the course of generative\\npre-training in Fig 2(right). We observe the performance of these heuristics is stable and steadily\\nincreases over training suggesting that generative pretraining supports the learning of a wide variety\\nof task relevant functionality. We also observe the LSTM exhibits higher variance in its zero-shot\\nperformance suggesting that the inductive bias of the Transformer architecture assists in transfer.\\nFor CoLA (linguistic acceptability), examples are scored as the average token log-probability the\\ngenerative model assigns and predictions are made by thresholding. For SST-2 (sentiment analysis),\\nwe append the token very to each example and restrict the language model’s output distribution to only\\nthe words positive and negative and guess the token it assigns higher probability to as the prediction.\\nFor RACE (question answering), we pick the answer the generative model assigns the highest average\\ntoken log-probability when conditioned on the document and question. For DPRD [46] (winograd\\nschemas), we replace the deﬁnite pronoun with the two possible referrents and predict the resolution'), Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 1, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='scheme, enabling better generalization in deep neural networks. In recent work, the method has\\nbeen used to help train deep neural networks on various tasks like image classiﬁcation [69], speech\\nrecognition [68], entity disambiguation [17] and machine translation [48].\\nThe closest line of work to ours involves pre-training a neural network using a language modeling\\nobjective and then ﬁne-tuning it on a target task with supervision. Dai et al. [13] and Howard and\\nRuder [21] follow this method to improve text classiﬁcation. However, although the pre-training\\nphase helps capture some linguistic information, their usage of LSTM models restricts their prediction\\nability to a short range. In contrast, our choice of transformer networks allows us to capture longer-\\nrange linguistic structure, as demonstrated in our experiments. Further, we also demonstrate the\\neffectiveness of our model on a wider range of tasks including natural language inference, paraphrase\\ndetection and story completion. Other approaches [43, 44, 38] use hidden representations from a\\n2'), Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 6, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='Table 4: Semantic similarity and classiﬁcation results, comparing our model with current state-of-the-\\nart methods. All task evaluations in this table were done using the GLUE benchmark. (mc= Mathews\\ncorrelation, acc=Accuracy, pc=Pearson correlation)\\nMethod\\nClassiﬁcation\\nSemantic Similarity\\nGLUE\\nCoLA\\nSST2\\nMRPC\\nSTSB\\nQQP\\n(mc)\\n(acc)\\n(F1)\\n(pc)\\n(F1)\\nSparse byte mLSTM [16]\\n-\\n93.2\\n-\\n-\\n-\\n-\\nTF-KLD [23]\\n-\\n-\\n86.0\\n-\\n-\\n-\\nECNU (mixed ensemble) [60]\\n-\\n-\\n-\\n81.0\\n-\\n-\\nSingle-task BiLSTM + ELMo + Attn [64]\\n35.0\\n90.2\\n80.2\\n55.5\\n66.1\\n64.8\\nMulti-task BiLSTM + ELMo + Attn [64]\\n18.9\\n91.6\\n83.5\\n72.8\\n63.3\\n68.9\\nFinetuned Transformer LM (ours)\\n45.4\\n91.3\\n82.3\\n82.0\\n70.3\\n72.8\\nOverall, our approach achieves new state-of-the-art results in 9 out of the 12 datasets we evaluate\\non, outperforming ensembles in many cases. Our results also indicate that our approach works well\\nacross datasets of different sizes, from smaller datasets such as STS-B (≈5.7k training examples) –\\nto the largest one – SNLI (≈550k training examples).\\n5\\nAnalysis\\nImpact of number of layers transferred\\nWe observed the impact of transferring a variable number\\nof layers from unsupervised pre-training to the supervised target task. Figure 2(left) illustrates the\\nperformance of our approach on MultiNLI and RACE as a function of the number of layers transferred.\\nWe observe the standard result that transferring embeddings improves performance and that each\\ntransformer layer provides further beneﬁts up to 9% for full transfer on MultiNLI. This indicates that\\neach layer in the pre-trained model contains useful functionality for solving target tasks.\\nFigure 2: (left) Effect of transferring increasing number of layers from the pre-trained language\\nmodel on RACE and MultiNLI. (right) Plot showing the evolution of zero-shot performance on\\ndifferent tasks as a function of LM pre-training updates. Performance per task is normalized between\\na random guess baseline and the current state-of-the-art with a single model.\\nZero-shot Behaviors')]\n"]}],"source":["# Getting the documents used as context\n","print(response[\"source_documents\"])"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"rQRCNcK9mIl1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","The attention mechanism in BERT and other transformer models allows the model to focus on the relevant parts of the input sequence when encoding a specific word or token. Here's a high-level overview of how it works:\n","\n","1. The input sequence is first converted into vector representations (embeddings) for each token.\n","\n","2. These embeddings go through multiple layers of self-attention, where each token attends to the other tokens in the sequence to compute its representation.\n","\n","3. In self-attention, three vectors are computed for each token - a query vector, a key vector, and a value vector. The query vector is compared against the key vectors of all other tokens using a similarity score (e.g. dot product). This produces attention weights that are higher for tokens more relevant to the current token.\n","\n","4. The value vectors of all tokens are then multiplied by their corresponding attention weights and summed up to produce the final self-attended representation for the current token.\n","\n","5. This self-attention calculation is done in parallel for all tokens, allowing the model to capture long-range dependencies and weigh the importance of other tokens when encoding each token representation.\n","\n","6. The self-attended representations are then passed through a feed-forward neural network to produce the output representations for that layer.\n","\n","7. This process repeats across multiple self-attention and feed-forward layers, allowing the representations to be refined across different subspaces.\n","\n","The multi-headed attention mechanism further allows the model to attend to different representation subspaces in parallel. This attention mechanism allows BERT to effectively model intricate relationships between words in a data-driven way, improving its language understanding capabilities.\n"]}],"source":["# Asking a new question and printing the answer\n","response = chain.invoke(\"How does the attention mechanism work?\")\n","print(response[\"result\"])"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"38bj3hE_mIl1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 4, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='Table 1: A list of the different tasks and datasets used in our experiments.\\nTask\\nDatasets\\nNatural language inference\\nSNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25]\\nQuestion Answering\\nRACE [30], Story Cloze [40]\\nSentence similarity\\nMSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6]\\nClassiﬁcation\\nStanford Sentiment Treebank-2 [54], CoLA [65]\\nbut is shufﬂed at a sentence level - destroying long-range structure. Our language model achieves a\\nvery low token level perplexity of 18.4 on this corpus.\\nModel speciﬁcations\\nOur model largely follows the original transformer work [62]. We trained a\\n12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12\\nattention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.\\nWe used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate\\nwas increased linearly from zero over the ﬁrst 2000 updates and annealed to 0 using a cosine schedule.\\nWe train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.\\nSince layernorm [2] is used extensively throughout the model, a simple weight initialization of\\nN(0, 0.02) was sufﬁcient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53]\\nand residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also\\nemployed a modiﬁed version of L2 regularization proposed in [37], with w = 0.01 on all non bias or\\ngain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]. We\\nused learned position embeddings instead of the sinusoidal version proposed in the original work.\\nWe use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and\\nwhitespace, and use the spaCy tokenizer.3\\nFine-tuning details\\nUnless speciﬁed, we reuse the hyperparameter settings from unsupervised'), Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 10, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='[46] A. Rahman and V. Ng. Resolving complex cases of deﬁnite pronouns: the winograd schema challenge. In\\nProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and\\nComputational Natural Language Learning, pages 777–789. Association for Computational Linguistics,\\n2012.\\n[47] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension\\nof text. EMNLP, 2016.\\n[48] P. Ramachandran, P. J. Liu, and Q. V. Le. Unsupervised pretraining for sequence to sequence learning.\\narXiv preprint arXiv:1611.02683, 2016.\\n[49] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with an\\nenergy-based model. In Advances in neural information processing systems, pages 1137–1144, 2007.\\n[50] M. Rei. Semi-supervised multitask learning for sequence labeling. ACL, 2017.\\n[51] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statistics,\\npages 400–407, 1951.\\n[52] T. Rocktäschel, E. Grefenstette, K. M. Hermann, T. Koˇcisk`y, and P. Blunsom. Reasoning about entailment\\nwith neural attention. arXiv preprint arXiv:1509.06664, 2015.\\n[53] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. arXiv\\npreprint arXiv:1508.07909, 2015.\\n[54] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for\\nsemantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical\\nmethods in natural language processing, pages 1631–1642, 2013.\\n[55] S. Srinivasan, R. Arora, and M. Riedl. A simple and effective approach to the story cloze test. arXiv\\npreprint arXiv:1803.05547, 2018.\\n[56] S. Subramanian, A. Trischler, Y. Bengio, and C. J. Pal. Learning general purpose distributed sentence\\nrepresentations via large scale multi-task learning. arXiv preprint arXiv:1804.00079, 2018.'), Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 8, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In\\nAdvances in neural information processing systems, pages 153–160, 2007.\\n[4] L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo. The ﬁfth pascal recognizing textual entailment\\nchallenge. In TAC, 2009.\\n[5] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning natural\\nlanguage inference. EMNLP, 2015.\\n[6] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. Semeval-2017 task 1: Semantic textual\\nsimilarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.\\n[7] S. Chaturvedi, H. Peng, and D. Roth. Story comprehension for predicting what happens next. In Proceedings\\nof the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1603–1614, 2017.\\n[8] D. Chen and C. Manning. A fast and accurate dependency parser using neural networks. In Proceedings\\nof the 2014 conference on empirical methods in natural language processing (EMNLP), pages 740–750,\\n2014.\\n[9] Z. Chen, H. Zhang, X. Zhang, and L. Zhao. Quora question pairs. https://data.quora.com/First-Quora-\\nDataset-Release-Question-Pairs, 2018.\\n[10] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep neural networks\\nwith multitask learning. In Proceedings of the 25th international conference on Machine learning, pages\\n160–167. ACM, 2008.\\n[11] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing\\n(almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493–2537, 2011.\\n[12] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes. Supervised learning of universal sentence\\nrepresentations from natural language inference data. EMNLP, 2017.'), Document(metadata={'source': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'file_path': '/Users/raquelcardoso/Documents/interns_ml_pt/Raquel/week5/4. Retrieval Agumented Generation (RAG)/gpt.pdf', 'page': 4, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'trapped': ''}, page_content='sentences, and handle aspects of linguistic ambiguity. On RTE, one of the smaller datasets we\\nevaluate on (2490 examples), we achieve an accuracy of 56%, which is below the 61.7% reported by a\\nmulti-task biLSTM model. Given the strong performance of our approach on larger NLI datasets, it is\\nlikely our model will beneﬁt from multi-task training as well but we have not explored this currently.\\n2https://ftfy.readthedocs.io/en/latest/\\n3https://spacy.io/\\n5')]\n"]}],"source":["# Getting the documents used as context\n","print(response[\"source_documents\"])"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"MLOps","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}
